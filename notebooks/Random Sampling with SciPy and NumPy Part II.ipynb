{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7721072f-6bd5-4c98-8f8a-6e8c0cfc4dec",
   "metadata": {},
   "source": [
    "# Random Sampling using SciPy and NumPy: Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b5c37-165a-4b9f-bcfd-0f48e846a777",
   "metadata": {},
   "source": [
    "## Fancy algorithms, source code walkthrough and potential improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170110e-b2ed-42b0-991c-0dff6b262855",
   "metadata": {},
   "source": [
    "In Part I we went through the basics of Inverse Transform Sampling (ITS) and created our own ITS pure python implementation to sample numbers from a standard normal distribution. We then compared the speed of our somewhat optimised function to that of the built in SciPy function and found ourselves somewhat lacking - to the tune of being `40x` slower.\n",
    "\n",
    "In this part the aim is to explain why that is the case by digging through the relevant bits of the SciPy and NumPy code base to see where those speed improvements manifest themselves. In general we will find that it's made up of a combination of:\n",
    " - faster functions either due to being written in Cython or straight C\n",
    " - faster newer sampling algorithms compared to our tried and tested Inverse Transform Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b870c8-d806-4d49-92f5-433242eb8648",
   "metadata": {},
   "source": [
    "## How do we generate normally distributed random samples in SciPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c1ed6-8398-49ac-b6f8-7f6165b14231",
   "metadata": {},
   "source": [
    "The following is the code to generate `1,000,000` random numbers from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eeb21f9-d610-4ddf-a8e5-988c408394e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.5 ms ± 1.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# usual suspects\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "\n",
    "# some config\n",
    "%config Completer.use_jedi = False\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set() \n",
    "\n",
    "# set size\n",
    "n = 1000000\n",
    "# init dist\n",
    "snorm = stats.norm()\n",
    "# generate samples\n",
    "samps = snorm.rvs(size=n)\n",
    "%timeit snorm.rvs(size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd2c4a-da65-450b-97c1-da77af6f215c",
   "metadata": {},
   "source": [
    "So the function `rvs` generates `1,000,000` samples in just over `40ms`. For comparison we were able to achieve this on average in `2.3s` using our algorithm which was based on the principle of inverse transform sampling. To understand the speed differences we're going to have to dive into that `rvs` method.\n",
    "\n",
    "It's worth noting that _(in general)_ with SciPy the core of the logic is contained in underscore methods - so when we want to have a look into `rvs` really we want to see the code for `_rvs`. The non-underscore methods generally implement some argument type checking or defaulting before handing over to the underscore methods.\n",
    "\n",
    "Before working our way through let's just do a brief overview of the way SciPy organises distribution functionality in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3f73c-d0dc-4d02-bad7-101bb28e2a95",
   "metadata": {},
   "source": [
    "## rv_generic and rv_continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00949737-41b7-4089-9f86-19b83b99ecfb",
   "metadata": {},
   "source": [
    "SciPy distributions are created from a neat inheritance structure with:\n",
    " - `rv_generic` as the top level class providing methods like `get_support` and `mean`\n",
    " - `rv_continuous` and `rv_discrete` inheriting from it with more specific methods\n",
    "\n",
    "So in the above case where we initiated our normal distribution class `snorm` as `stats.norm()` what that is really doing is creating an instance of `rv_continuous` which inherits a lot of functionality from `rv_generic`. To be even more specific, we actually create an `rv_frozen` instance which is a version of `rv_continuous` but with the params of the distribution fixed (e.g. the mean and variance). With that in mind, let's now peer inside the `rvs` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8593f4-72d5-4cab-a52e-b27b612b2f79",
   "metadata": {},
   "source": [
    "## rvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4eb56-cb46-4833-98b5-077e711e7669",
   "metadata": {},
   "source": [
    "When we run the `??` magic on `snorm.dist._rvs` we see the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5261e856-5dca-4e63-8607-463d93a8ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? snorm.dist._rvs\n",
    "def _rvs(self, size=None, random_state=None):\n",
    "    return random_state.standard_normal(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a740b-f4be-4f8e-a74a-fa3f38856cb2",
   "metadata": {},
   "source": [
    "So it seems like somewhere in the distribution class we created we have assigned a `random_state` object somewhere and that `random_state` object contains a method that can return numbers distributed according to a standard normal distribution.\n",
    "\n",
    "__It turns out that the `random_state` object that spits out these random numbers is actually from NumPy.__ We see this by looking at the source code for [rv_generic](https://github.com/scipy/scipy/blob/b5d8bab88af61d61de09641243848df63380a67f/scipy/stats/_distn_infrastructure.py#L627) which contains in its `__init__` method a call to a SciPy util method called [check_random_state](https://github.com/scipy/scipy/blob/e3cd846ef353b10cc66972a5c7718e80948362ac/scipy/_lib/_util.py#L209) which, if no seed is passed already, will set the `random_state` as an instance of `np.random.RandomState`. Below is this code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1387e70d-0b6f-4281-b050-41d21846e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripped out the doc string for ease\n",
    "def check_random_state(seed):\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    \n",
    "    ### this is the key line ###\n",
    "    if isinstance(seed, (numbers.Integral, np.integer)):\n",
    "        return np.random.RandomState(seed)\n",
    "    \n",
    "    if isinstance(seed, (np.random.RandomState, np.random.Generator)):\n",
    "        return seed\n",
    "\n",
    "    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'\n",
    "                     ' instance' % seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc1d95-63c3-4905-b17b-1d9bd41fc417",
   "metadata": {},
   "source": [
    "## Over to NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e458f6-cb35-4c3e-86eb-386c2795cbc0",
   "metadata": {},
   "source": [
    "So it seems like the 'magic' that delivers such blazing fast sampling actually sits in NumPy, not SciPy. This shouldn't be all that shocking as SciPy is deliberately built on top of NumPy to prevent duplication and inconsistencies where the two libraries may provide identical features. This is explicitly stated in the first line of the SciPy Intro documentation [here](https://docs.scipy.org/doc/scipy/tutorial/general.html):\n",
    "\n",
    "_\"SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python.\"_\n",
    "\n",
    "To see what is going on we can have a look at the `np.random.RandomState` class [here](https://github.com/numpy/numpy/blob/b991d0992a56272531e18613cc26b0ba085459ef/numpy/random/mtrand.pyx#L120). We can see from the use of:\n",
    " - `cdef` instead of `def` for function declaration\n",
    " - a `.pyx` file extension instead of .py\n",
    "\n",
    "which both indicate that the function is written using [Cython](https://cython.readthedocs.io/en/latest/index.html) - a language very similar to Python that allows functions to be written in almost python syntax, but then compiled into optimised C/C++ code for efficiency. As they put it themselves [in the documentation](https://cython.readthedocs.io/en/latest/src/quickstart/overview.html):\n",
    "\n",
    "_\"The source code gets translated into optimised C/C++ code and compiled as Python extension modules. This allows for both very fast program execution and tight integration with external C libraries, while keeping up the high programmer productivity for which the Python language is well known.\"_\n",
    "\n",
    "Within this class there are two things we need to look at to understand the sampling process:\n",
    " - what it is doing to generate the uniformly distributed random numbers (the PRNG)\n",
    " - what algorithm it is using to convert these uniformly distributed numbers into normally distributed numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2fc97f-354f-40f0-9b93-e93d37280734",
   "metadata": {},
   "source": [
    "## The PRNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d23daf-1c60-49f6-872f-9854c2ce06c3",
   "metadata": {},
   "source": [
    "As mentioned in Part I, generating a random sample requires some form of randomness. Almost always this isn't _true_ randomness, but a series of numbers generated by a 'pseudo-random number generator' (PRNG). Just as with sampling algorithms, there are a variety of PRNGs available and the specific implementation used here is [detailed in the `__init__` method](https://github.com/numpy/numpy/blob/b991d0992a56272531e18613cc26b0ba085459ef/numpy/random/mtrand.pyx#L180) of `np.random.RandomState`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79447c1f-64fb-4dad-ad03-3a9a64400510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, seed=None):\n",
    "    if seed is None:\n",
    "        bit_generator = _MT19937()\n",
    "    elif not hasattr(seed, 'capsule'):\n",
    "        bit_generator = _MT19937()\n",
    "        bit_generator._legacy_seeding(seed)\n",
    "    else:\n",
    "        bit_generator = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6740b-1995-4ffa-abc9-c09cee1c16e5",
   "metadata": {},
   "source": [
    "As the above shows, when the class is initiated, the default PRNG is set to be an implementation of the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) algorithm - named as such as it has a period length of a [Mersenne prime](https://en.wikipedia.org/wiki/Mersenne_prime) (the number of random numbers it can generate before it starts to repeat itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aefc87-d3e7-492d-9d67-916edece09dc",
   "metadata": {},
   "source": [
    "## The Sampling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70427e6-f4c8-413e-93d3-994ea4c799ce",
   "metadata": {},
   "source": [
    "Some way down the code for the class `np.random.RandomState` we see [the definition of `standard_normal`](https://github.com/numpy/numpy/blob/b991d0992a56272531e18613cc26b0ba085459ef/numpy/random/mtrand.pyx#L1344) making a call to something called `legacy_gauss`. The C code for the `legacy_gauss` function is [here](https://github.com/numpy/numpy/blob/b991d0992a56272531e18613cc26b0ba085459ef/numpy/random/src/legacy/legacy-distributions.c#L18) and for ease of viewing we'll show it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d206d9a-146f-441b-b536-5e40b48ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "double legacy_gauss(aug_bitgen_t *aug_state) {\n",
    "  if (aug_state->has_gauss) {\n",
    "    const double temp = aug_state->gauss;\n",
    "    aug_state->has_gauss = false;\n",
    "    aug_state->gauss = 0.0;\n",
    "    return temp;\n",
    "  } else {\n",
    "    double f, x1, x2, r2;\n",
    "\n",
    "    do {\n",
    "      x1 = 2.0 * legacy_double(aug_state) - 1.0;\n",
    "      x2 = 2.0 * legacy_double(aug_state) - 1.0;\n",
    "      r2 = x1 * x1 + x2 * x2;\n",
    "    } while (r2 >= 1.0 || r2 == 0.0);\n",
    "\n",
    "    /* Polar method, a more efficient version of the Box-Muller approach. */\n",
    "    f = sqrt(-2.0 * log(r2) / r2);\n",
    "    /* Keep for next call */\n",
    "    aug_state->gauss = f * x1;\n",
    "    aug_state->has_gauss = true;\n",
    "    return f * x2;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a2173-a037-4c62-9ff3-2c31d6ff1cc4",
   "metadata": {},
   "source": [
    "As can be seen on Wiki in the [implementation section](https://en.wikipedia.org/wiki/Marsaglia_polar_method#Implementation), this is none other than a C implementation of the [Marsaglia Polar Method](https://en.wikipedia.org/wiki/Marsaglia_polar_method#Implementation) for generating random samples from a normal distribution given a stream of uniformly distributed input numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dfa0-c4fe-4d83-9ebf-35d6f2d5173f",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49488e-97ee-46f6-9b2b-b3003b67cb97",
   "metadata": {},
   "source": [
    "We've gone through a lot there so it's worth stepping back through and making sure everything is crystal clear. We've gone from:\n",
    " - a SciPy function called `_rvs`, written in python, initiates\n",
    " - a NumPy class `np.random.RandomState`, written in Cython, which\n",
    " - generates uniformly distributed numbers using the Mersenne Twister algorithm and then\n",
    " - feeds these numbers into a function `legacy_gauss`, written in C, which churns out normally distributed samples using the Marsaglia Polar method\n",
    "\n",
    "The above highlights the lengths that the clever people building SciPy and NumPy have gone to to generate efficient code. We have a top layer callable by users (like you and me) that is written in python (for the 'programmer productivity' of python) before deeper layers of the infrastructure are increasingly written as close to C as possible (for speed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee78b9f-44b5-4ac5-8753-ee9fbfd921a8",
   "metadata": {},
   "source": [
    "## Why is SciPy calling a NumPy function deemed 'legacy'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d8cab-cbf3-4e98-b890-37f75a166a2a",
   "metadata": {},
   "source": [
    "Because sampling is a branch of maths / computer science that is still moving forward. Unlike other areas where certain principles were agreed upon centuries ago and haven't seen change since, efficiently sampling various distributions is still seeing fresh developments. As new developments get tested, we would like to update our default processes to incorporate these advancements.\n",
    "\n",
    "This is exactly what happened in July 2019 with NumPy 1.17.0 when [they introduced 2 new features that impact sampling](https://numpy.org/devdocs/release/1.17.0-notes.html):\n",
    " - the implementation of a new default pseudo-random number generator (PRNG): [Melissa O'Neil's PCG family of algorithms](https://www.pcg-random.org/index.html)\n",
    " - the implementation of a new sampling process: the [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm)\n",
    "\n",
    "Due to the desire for backward compatibility of PRNGs however, instead of creating a breaking change they introduced a new way to initiate PRNGs and switched the old way over to reference the 'legacy' code.\n",
    "\n",
    "The backward compatibility referenced here is the desire for a PRNG function to generate the same string of random numbers given the same seed. Two different algorithms will not produce the same random numbers even if they are given the same seed. This reproducibility is important especially for testing.\n",
    "\n",
    "It appears SciPy hasn't been upgraded yet to make use of these new developments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79e320-4280-4904-8820-d49fc4aa7158",
   "metadata": {},
   "source": [
    "## Can we beat SciPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3f059-7838-44f1-94af-939b357c5106",
   "metadata": {},
   "source": [
    "Given we know what we know now about how normal distribution sampling is implemented in SciPy, can we beat it?\n",
    "\n",
    "The answer is yes - by making use of the latest developments in sampling implemented for us in NumPy. Below is an implementation of sampling where we:\n",
    " - use the latest PRNG\n",
    " - use the new ziggurat algorithm for converting these numbers into a normally distributed sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db40bc1-7edb-4435-bdfd-c95a2f0ed941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "n = 1000000\n",
    "# define scipy implementation\n",
    "snorm = stats.norm()\n",
    "# define newer numpy implementation\n",
    "nnorm = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1110cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 ms ± 5.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# test scipy speed\n",
    "%timeit snorm.rvs(size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7893914a-80bf-4fe0-8607-2f6e4f382b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# test numpy speed\n",
    "%timeit nnorm.normal(size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bccb1-24d6-4b36-89e2-c701bc3324c7",
   "metadata": {},
   "source": [
    "So it seems like we're around `2x` as fast as SciPy now - something that is in the expected 2-10x bracket as NumPy highlights in their release [here](https://numpy.org/doc/stable/reference/random/index.html#what-s-new-or-different)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b554c91-d3ad-46d6-b676-35c58d519352",
   "metadata": {},
   "source": [
    "## Conclusion: how useful is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d98093-bdce-4175-91c7-8a9e41b5ed4f",
   "metadata": {},
   "source": [
    "When it comes to implementing custom distribution sampling: very useful. We now fully understand the decision to pursue SciPy-esque sampling speed and can implement custom distribution sampling appropriately. We can either:\n",
    " - stick with the pure python inverse sampling transform implementation in Part I (after all, `2s` isn't bad for a sample of `1,000,000` in most contexts)\n",
    " - write our own sampling procedure - and preferably write this sampling procedure in C or Cython - which is no small ask\n",
    "\n",
    "In the next part we'll look at doing just that - implement an efficient custom distribution sampling function within the SciPy infrastructure. This gives us the best of both worlds - the flexibility to implement the exact distribution of our choice along with making use of the efficient and well written methods that we inherit from the `rv_generic` and `rv_continuous` SciPy classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
